{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# E2E Predictive MLOps Demo -- Fraud Detection\n",
    "\n",
    "This notebook covers the **exploration and local iteration** phase of the MLOps lifecycle.\n",
    "We go from raw transaction data in BigQuery to a trained XGBoost fraud detection model,\n",
    "logging everything to Vertex AI Experiments.\n",
    "\n",
    "**Dataset**: FraudFinder (public fraud detection dataset in BigQuery)  \n",
    "**Tables**:\n",
    "- `tx` -- raw transactions (`tx_id`, `tx_ts`, `customer_id`, `terminal_id`, `tx_amount`)\n",
    "- `txlabels` -- fraud labels (`tx_id`, `tx_fraud`)\n",
    "- `demographics.customers` -- customer demographics\n",
    "- `demographics.terminals` -- terminal demographics\n",
    "\n",
    "**Sections**:\n",
    "1. Connect to BigQuery\n",
    "2. Exploratory Data Analysis (EDA)\n",
    "3. Feature Engineering\n",
    "4. Write Features to BigQuery\n",
    "5. Train Model Locally\n",
    "6. Iterate (hyperparameter tuning)\n",
    "7. Log to Vertex AI Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.1 Connect to BigQuery\n",
    "\n",
    "Authenticate, set project/dataset, and explore the raw tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import warnings\n",
    "\n",
    "import google.auth\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import xgboost as xgb\n",
    "from google.cloud import bigquery\n",
    "from sklearn.metrics import (\n",
    "    ConfusionMatrixDisplay,\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    precision_recall_curve,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set_theme(style=\"whitegrid\")\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# Project ID: set via environment variable or auto-detect from gcloud/ADC\n",
    "PROJECT_ID = os.environ.get(\"PROJECT_ID\") or os.environ.get(\"GOOGLE_CLOUD_PROJECT\")\n",
    "if PROJECT_ID is None:\n",
    "    _, PROJECT_ID = google.auth.default()\n",
    "    if PROJECT_ID is None:\n",
    "        PROJECT_ID = \"asp-test-dev\"  # fallback default for testing\n",
    "\n",
    "BQ_DATASET = \"fraud_detection\"\n",
    "FEATURES_DATASET = \"features\"\n",
    "FEATURES_TABLE = \"fraud_features\"\n",
    "REGION = \"us-central1\"\n",
    "\n",
    "print(f\"Project ID: {PROJECT_ID}\")\n",
    "print(f\"BQ Dataset: {BQ_DATASET}\")\n",
    "print(f\"Region:     {REGION}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BigQuery client\n",
    "bq_client = bigquery.Client(project=PROJECT_ID)\n",
    "\n",
    "# List tables in the fraud_detection dataset\n",
    "dataset_ref = bq_client.dataset(BQ_DATASET)\n",
    "tables = list(bq_client.list_tables(dataset_ref))\n",
    "print(f\"Tables in {BQ_DATASET}:\")\n",
    "for table in tables:\n",
    "    print(f\"  - {table.table_id}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the tx table\n",
    "query_tx_preview = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.tx`\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df_tx_preview = bq_client.query(query_tx_preview).to_dataframe()\n",
    "print(f\"tx table preview ({len(df_tx_preview)} rows):\")\n",
    "df_tx_preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the txlabels table\n",
    "query_labels_preview = f\"\"\"\n",
    "SELECT *\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.txlabels`\n",
    "LIMIT 10\n",
    "\"\"\"\n",
    "df_labels_preview = bq_client.query(query_labels_preview).to_dataframe()\n",
    "print(f\"txlabels table preview ({len(df_labels_preview)} rows):\")\n",
    "df_labels_preview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check table sizes\n",
    "for table_name in [\"tx\", \"txlabels\"]:\n",
    "    query_count = f\"SELECT COUNT(*) as cnt FROM `{PROJECT_ID}.{BQ_DATASET}.{table_name}`\"\n",
    "    result = bq_client.query(query_count).to_dataframe()\n",
    "    print(f\"{table_name}: {result['cnt'].iloc[0]:,} rows\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.2 Exploratory Data Analysis (EDA)\n",
    "\n",
    "Understand the data: basic statistics, class imbalance, and temporal patterns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a manageable sample of transactions joined with labels for EDA\n",
    "query_eda = f\"\"\"\n",
    "SELECT\n",
    "    t.tx_id,\n",
    "    t.tx_ts,\n",
    "    t.customer_id,\n",
    "    t.terminal_id,\n",
    "    t.tx_amount,\n",
    "    l.tx_fraud\n",
    "FROM `{PROJECT_ID}.{BQ_DATASET}.tx` AS t\n",
    "JOIN `{PROJECT_ID}.{BQ_DATASET}.txlabels` AS l\n",
    "    ON t.tx_id = l.tx_id\n",
    "\"\"\"\n",
    "print(\"Loading data from BigQuery (this may take a minute)...\")\n",
    "df = bq_client.query(query_eda).to_dataframe()\n",
    "print(f\"Loaded {len(df):,} transactions\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic statistics\n",
    "print(\"=\" * 60)\n",
    "print(\"BASIC STATISTICS\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"Date range: {df['tx_ts'].min()} to {df['tx_ts'].max()}\")\n",
    "print(f\"Unique customers: {df['customer_id'].nunique():,}\")\n",
    "print(f\"Unique terminals: {df['terminal_id'].nunique():,}\")\n",
    "print(f\"\\nTransaction amount statistics:\")\n",
    "df[\"tx_amount\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class imbalance check\n",
    "fraud_counts = df[\"tx_fraud\"].value_counts()\n",
    "fraud_pct = df[\"tx_fraud\"].value_counts(normalize=True) * 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"CLASS DISTRIBUTION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nLegitimate (0): {fraud_counts[0]:>10,}  ({fraud_pct[0]:.2f}%)\")\n",
    "print(f\"Fraudulent (1): {fraud_counts[1]:>10,}  ({fraud_pct[1]:.2f}%)\")\n",
    "print(f\"Imbalance ratio: 1:{fraud_counts[0] // fraud_counts[1]}\")\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "\n",
    "# Bar chart\n",
    "axes[0].bar([\"Legitimate\", \"Fraudulent\"], fraud_counts.values, color=[\"steelblue\", \"coral\"])\n",
    "axes[0].set_title(\"Transaction Count by Class\")\n",
    "axes[0].set_ylabel(\"Count\")\n",
    "for i, v in enumerate(fraud_counts.values):\n",
    "    axes[0].text(i, v + v * 0.01, f\"{v:,}\", ha=\"center\", fontweight=\"bold\")\n",
    "\n",
    "# Pie chart\n",
    "axes[1].pie(fraud_counts.values, labels=[\"Legitimate\", \"Fraudulent\"],\n",
    "            autopct=\"%1.2f%%\", colors=[\"steelblue\", \"coral\"], startangle=90)\n",
    "axes[1].set_title(\"Class Distribution\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction amount distribution by class\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "for ax, label, title in zip(\n",
    "    axes, [0, 1], [\"Legitimate Transactions\", \"Fraudulent Transactions\"]\n",
    "):\n",
    "    subset = df[df[\"tx_fraud\"] == label][\"tx_amount\"]\n",
    "    ax.hist(subset, bins=50, color=\"steelblue\" if label == 0 else \"coral\", edgecolor=\"white\")\n",
    "    ax.set_title(f\"{title} (n={len(subset):,})\")\n",
    "    ax.set_xlabel(\"Transaction Amount\")\n",
    "    ax.set_ylabel(\"Count\")\n",
    "    ax.axvline(subset.mean(), color=\"black\", linestyle=\"--\", label=f\"Mean: {subset.mean():.2f}\")\n",
    "    ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"Amount statistics by class:\")\n",
    "df.groupby(\"tx_fraud\")[\"tx_amount\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal distribution of fraud\n",
    "df[\"tx_date\"] = pd.to_datetime(df[\"tx_ts\"]).dt.date\n",
    "\n",
    "daily_fraud = df.groupby(\"tx_date\").agg(\n",
    "    total_tx=pd.NamedAgg(column=\"tx_id\", aggfunc=\"count\"),\n",
    "    fraud_tx=pd.NamedAgg(column=\"tx_fraud\", aggfunc=\"sum\"),\n",
    ").reset_index()\n",
    "daily_fraud[\"fraud_rate\"] = daily_fraud[\"fraud_tx\"] / daily_fraud[\"total_tx\"] * 100\n",
    "\n",
    "fig, axes = plt.subplots(2, 1, figsize=(14, 8), sharex=True)\n",
    "\n",
    "axes[0].plot(daily_fraud[\"tx_date\"], daily_fraud[\"total_tx\"], color=\"steelblue\", alpha=0.7)\n",
    "axes[0].fill_between(daily_fraud[\"tx_date\"], daily_fraud[\"total_tx\"], alpha=0.3, color=\"steelblue\")\n",
    "axes[0].set_title(\"Daily Transaction Volume\")\n",
    "axes[0].set_ylabel(\"Number of Transactions\")\n",
    "\n",
    "axes[1].plot(daily_fraud[\"tx_date\"], daily_fraud[\"fraud_rate\"], color=\"coral\", alpha=0.7)\n",
    "axes[1].fill_between(daily_fraud[\"tx_date\"], daily_fraud[\"fraud_rate\"], alpha=0.3, color=\"coral\")\n",
    "axes[1].set_title(\"Daily Fraud Rate (%)\")\n",
    "axes[1].set_ylabel(\"Fraud Rate (%)\")\n",
    "axes[1].set_xlabel(\"Date\")\n",
    "\n",
    "plt.xticks(rotation=45)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fraud by hour of day\n",
    "df[\"tx_hour\"] = pd.to_datetime(df[\"tx_ts\"]).dt.hour\n",
    "\n",
    "hourly_fraud = df.groupby(\"tx_hour\").agg(\n",
    "    total_tx=pd.NamedAgg(column=\"tx_id\", aggfunc=\"count\"),\n",
    "    fraud_tx=pd.NamedAgg(column=\"tx_fraud\", aggfunc=\"sum\"),\n",
    ").reset_index()\n",
    "hourly_fraud[\"fraud_rate\"] = hourly_fraud[\"fraud_tx\"] / hourly_fraud[\"total_tx\"] * 100\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4))\n",
    "ax.bar(hourly_fraud[\"tx_hour\"], hourly_fraud[\"fraud_rate\"], color=\"coral\", edgecolor=\"white\")\n",
    "ax.set_title(\"Fraud Rate by Hour of Day\")\n",
    "ax.set_xlabel(\"Hour\")\n",
    "ax.set_ylabel(\"Fraud Rate (%)\")\n",
    "ax.set_xticks(range(24))\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.3 Feature Engineering (Python)\n",
    "\n",
    "Compute rolling window features for each customer and terminal:\n",
    "- `count_tx_Xd` -- number of transactions in the past X days\n",
    "- `avg_tx_amount_Xd` -- average transaction amount in the past X days\n",
    "- `max_tx_amount_Xd` -- maximum transaction amount in the past X days\n",
    "\n",
    "Windows: **1 day, 7 days, 28 days, 90 days**\n",
    "\n",
    "> **Note**: This uses pandas for feature engineering. The same logic can be implemented\n",
    "> using SQL in BigQuery, BigFrames, or PySpark on Dataproc. The choice is independent\n",
    "> of the rest of the MLOps stack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure tx_ts is datetime and sort by timestamp\n",
    "df[\"tx_ts\"] = pd.to_datetime(df[\"tx_ts\"])\n",
    "df = df.sort_values(\"tx_ts\").reset_index(drop=True)\n",
    "\n",
    "print(f\"Data sorted by timestamp: {df['tx_ts'].min()} to {df['tx_ts'].max()}\")\n",
    "print(f\"Total rows: {len(df):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_rolling_features(df: pd.DataFrame, group_col: str, windows_days: list[int]) -> pd.DataFrame:\n",
    "    \"\"\"Compute rolling window features (count, avg, max) for a given grouping column.\n",
    "\n",
    "    Args:\n",
    "        df: DataFrame with tx_ts, tx_amount, and the group_col.\n",
    "        group_col: Column to group by (e.g., 'customer_id' or 'terminal_id').\n",
    "        windows_days: List of window sizes in days.\n",
    "\n",
    "    Returns:\n",
    "        DataFrame with rolling features added as new columns.\n",
    "    \"\"\"\n",
    "    # Work on a copy sorted by group and timestamp\n",
    "    result = df.copy()\n",
    "    result = result.sort_values([group_col, \"tx_ts\"]).reset_index(drop=True)\n",
    "\n",
    "    # Set tx_ts as index for rolling operations\n",
    "    group_suffix = group_col.replace(\"_id\", \"\")\n",
    "\n",
    "    for window in windows_days:\n",
    "        print(f\"  Computing {window}d window for {group_col}...\")\n",
    "        window_str = f\"{window}D\"\n",
    "\n",
    "        # Group by the entity and compute rolling stats\n",
    "        grouped = result.set_index(\"tx_ts\").groupby(group_col)[\"tx_amount\"]\n",
    "\n",
    "        rolling = grouped.rolling(window_str, min_periods=1)\n",
    "\n",
    "        count_col = f\"count_tx_{window}d_{group_suffix}\"\n",
    "        avg_col = f\"avg_tx_amount_{window}d_{group_suffix}\"\n",
    "        max_col = f\"max_tx_amount_{window}d_{group_suffix}\"\n",
    "\n",
    "        counts = rolling.count().reset_index(level=0, drop=True).rename(count_col)\n",
    "        avgs = rolling.mean().reset_index(level=0, drop=True).rename(avg_col)\n",
    "        maxs = rolling.max().reset_index(level=0, drop=True).rename(max_col)\n",
    "\n",
    "        # Join back -- align on the original index\n",
    "        for series in [counts, avgs, maxs]:\n",
    "            result = result.set_index(\"tx_ts\")\n",
    "            result[series.name] = series\n",
    "            result = result.reset_index()\n",
    "\n",
    "    return result\n",
    "\n",
    "\n",
    "WINDOWS = [1, 7, 28, 90]\n",
    "print(f\"Windows: {WINDOWS} days\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute customer-level rolling features\n",
    "print(\"Computing customer-level rolling features...\")\n",
    "df_features = compute_rolling_features(df, \"customer_id\", WINDOWS)\n",
    "print(f\"Done. Shape: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute terminal-level rolling features\n",
    "print(\"Computing terminal-level rolling features...\")\n",
    "df_features = compute_rolling_features(df_features, \"terminal_id\", WINDOWS)\n",
    "print(f\"Done. Shape: {df_features.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop temporary columns and inspect the feature set\n",
    "df_features = df_features.drop(columns=[\"tx_date\", \"tx_hour\"], errors=\"ignore\")\n",
    "\n",
    "print(f\"Feature table shape: {df_features.shape}\")\n",
    "print(f\"\\nColumns:\")\n",
    "for col in df_features.columns:\n",
    "    print(f\"  - {col}: {df_features[col].dtype}\")\n",
    "\n",
    "print(f\"\\nNull counts:\")\n",
    "print(df_features.isnull().sum())\n",
    "\n",
    "df_features.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature correlation heatmap\n",
    "feature_cols = [c for c in df_features.columns if c.startswith((\"count_\", \"avg_\", \"max_\"))] + [\"tx_amount\"]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "corr = df_features[feature_cols + [\"tx_fraud\"]].corr()\n",
    "sns.heatmap(corr, annot=True, fmt=\".2f\", cmap=\"RdBu_r\", center=0,\n",
    "            square=True, linewidths=0.5, ax=ax)\n",
    "ax.set_title(\"Feature Correlation Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.4 Write Features Back to BigQuery\n",
    "\n",
    "Write the engineered feature table to `features.fraud_features` so BigQuery remains\n",
    "the single source of truth for both raw data and features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the features dataset if it does not exist\n",
    "features_dataset_ref = bigquery.DatasetReference(PROJECT_ID, FEATURES_DATASET)\n",
    "try:\n",
    "    bq_client.get_dataset(features_dataset_ref)\n",
    "    print(f\"Dataset '{FEATURES_DATASET}' already exists.\")\n",
    "except Exception:\n",
    "    dataset = bigquery.Dataset(features_dataset_ref)\n",
    "    dataset.location = \"US\"\n",
    "    bq_client.create_dataset(dataset)\n",
    "    print(f\"Created dataset '{FEATURES_DATASET}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the feature table to BigQuery\n",
    "destination_table = f\"{PROJECT_ID}.{FEATURES_DATASET}.{FEATURES_TABLE}\"\n",
    "\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=bigquery.WriteDisposition.WRITE_TRUNCATE,  # overwrite\n",
    ")\n",
    "\n",
    "print(f\"Writing {len(df_features):,} rows to {destination_table}...\")\n",
    "job = bq_client.load_table_from_dataframe(df_features, destination_table, job_config=job_config)\n",
    "job.result()  # wait for completion\n",
    "\n",
    "# Verify\n",
    "table = bq_client.get_table(destination_table)\n",
    "print(f\"Written successfully: {table.num_rows:,} rows, {table.num_bytes:,} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.5 Train Model Locally\n",
    "\n",
    "- Time-based train/test split (no future data leakage)\n",
    "- Train XGBoost classifier\n",
    "- Evaluate: precision, recall, AUC-ROC, confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define feature columns and label\n",
    "FEATURE_COLS = [c for c in df_features.columns if c.startswith((\"count_\", \"avg_\", \"max_\"))] + [\"tx_amount\"]\n",
    "LABEL_COL = \"tx_fraud\"\n",
    "\n",
    "print(f\"Number of features: {len(FEATURE_COLS)}\")\n",
    "print(f\"Features: {FEATURE_COLS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time-based train/test split\n",
    "# Use the split date from config (or compute one based on the data)\n",
    "SPLIT_DATE = pd.Timestamp(\"2023-06-01\")\n",
    "\n",
    "# If the data does not span this date, use a 80/20 time-based split\n",
    "if SPLIT_DATE < df_features[\"tx_ts\"].min() or SPLIT_DATE > df_features[\"tx_ts\"].max():\n",
    "    SPLIT_DATE = df_features[\"tx_ts\"].quantile(0.8)\n",
    "    print(f\"Adjusted split date to 80th percentile: {SPLIT_DATE}\")\n",
    "\n",
    "train_mask = df_features[\"tx_ts\"] < SPLIT_DATE\n",
    "test_mask = df_features[\"tx_ts\"] >= SPLIT_DATE\n",
    "\n",
    "X_train = df_features.loc[train_mask, FEATURE_COLS].fillna(0)\n",
    "y_train = df_features.loc[train_mask, LABEL_COL]\n",
    "X_test = df_features.loc[test_mask, FEATURE_COLS].fillna(0)\n",
    "y_test = df_features.loc[test_mask, LABEL_COL]\n",
    "\n",
    "print(f\"Split date: {SPLIT_DATE}\")\n",
    "print(f\"Train set: {len(X_train):,} rows ({y_train.mean()*100:.2f}% fraud)\")\n",
    "print(f\"Test set:  {len(X_test):,} rows ({y_test.mean()*100:.2f}% fraud)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train XGBoost classifier\n",
    "# Compute scale_pos_weight to handle class imbalance\n",
    "neg_count = (y_train == 0).sum()\n",
    "pos_count = (y_train == 1).sum()\n",
    "scale_pos_weight = neg_count / pos_count if pos_count > 0 else 1.0\n",
    "\n",
    "xgb_params = {\n",
    "    \"max_depth\": 6,\n",
    "    \"n_estimators\": 200,\n",
    "    \"learning_rate\": 0.1,\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "print(\"Training XGBoost with parameters:\")\n",
    "for k, v in xgb_params.items():\n",
    "    print(f\"  {k}: {v}\")\n",
    "\n",
    "model = xgb.XGBClassifier(**xgb_params)\n",
    "model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    verbose=20,\n",
    ")\n",
    "print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predictions and probabilities\n",
    "y_pred = model.predict(X_test)\n",
    "y_prob = model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# AUC-ROC\n",
    "auc_roc = roc_auc_score(y_test, y_prob)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nAUC-ROC: {auc_roc:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=[\"Legitimate\", \"Fraud\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion matrix\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Confusion matrix\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "ConfusionMatrixDisplay(cm, display_labels=[\"Legitimate\", \"Fraud\"]).plot(ax=axes[0], cmap=\"Blues\")\n",
    "axes[0].set_title(\"Confusion Matrix\")\n",
    "\n",
    "# ROC curve\n",
    "fpr, tpr, _ = roc_curve(y_test, y_prob)\n",
    "axes[1].plot(fpr, tpr, color=\"coral\", lw=2, label=f\"AUC = {auc_roc:.4f}\")\n",
    "axes[1].plot([0, 1], [0, 1], color=\"gray\", linestyle=\"--\")\n",
    "axes[1].set_xlabel(\"False Positive Rate\")\n",
    "axes[1].set_ylabel(\"True Positive Rate\")\n",
    "axes[1].set_title(\"ROC Curve\")\n",
    "axes[1].legend(loc=\"lower right\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Precision-Recall curve (more informative for imbalanced classes)\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_prob)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(recall, precision, color=\"coral\", lw=2)\n",
    "ax.set_xlabel(\"Recall\")\n",
    "ax.set_ylabel(\"Precision\")\n",
    "ax.set_title(\"Precision-Recall Curve\")\n",
    "ax.set_xlim([0, 1])\n",
    "ax.set_ylim([0, 1])\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "importance = model.feature_importances_\n",
    "importance_df = pd.DataFrame({\n",
    "    \"feature\": FEATURE_COLS,\n",
    "    \"importance\": importance,\n",
    "}).sort_values(\"importance\", ascending=True)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "ax.barh(importance_df[\"feature\"], importance_df[\"importance\"], color=\"steelblue\")\n",
    "ax.set_xlabel(\"Feature Importance (Gain)\")\n",
    "ax.set_title(\"XGBoost Feature Importance\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.6 Iterate\n",
    "\n",
    "Tweak features, hyperparameters, and re-run cells to improve the model.\n",
    "This section demonstrates the fast feedback loop -- change parameters and see results immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Iteration example: try different hyperparameters ---\n",
    "# Modify these and re-run to see the impact.\n",
    "\n",
    "iteration_params = [\n",
    "    {\"max_depth\": 4, \"n_estimators\": 100, \"learning_rate\": 0.05},\n",
    "    {\"max_depth\": 6, \"n_estimators\": 200, \"learning_rate\": 0.1},\n",
    "    {\"max_depth\": 8, \"n_estimators\": 300, \"learning_rate\": 0.1},\n",
    "    {\"max_depth\": 6, \"n_estimators\": 500, \"learning_rate\": 0.05},\n",
    "]\n",
    "\n",
    "results = []\n",
    "for i, params in enumerate(iteration_params):\n",
    "    full_params = {\n",
    "        **params,\n",
    "        \"scale_pos_weight\": scale_pos_weight,\n",
    "        \"eval_metric\": \"auc\",\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"random_state\": 42,\n",
    "        \"n_jobs\": -1,\n",
    "    }\n",
    "    m = xgb.XGBClassifier(**full_params)\n",
    "    m.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "    y_prob_iter = m.predict_proba(X_test)[:, 1]\n",
    "    auc = roc_auc_score(y_test, y_prob_iter)\n",
    "\n",
    "    results.append({\"run\": i + 1, **params, \"auc_roc\": auc})\n",
    "    print(f\"Run {i+1}: max_depth={params['max_depth']}, n_estimators={params['n_estimators']}, \"\n",
    "          f\"lr={params['learning_rate']} -> AUC: {auc:.4f}\")\n",
    "\n",
    "results_df = pd.DataFrame(results)\n",
    "print(\"\\nAll results:\")\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select the best model from the iteration\n",
    "best_idx = results_df[\"auc_roc\"].idxmax()\n",
    "best_params = results_df.iloc[best_idx]\n",
    "print(f\"Best run: #{int(best_params['run'])} with AUC-ROC: {best_params['auc_roc']:.4f}\")\n",
    "print(f\"Parameters: max_depth={int(best_params['max_depth'])}, \"\n",
    "      f\"n_estimators={int(best_params['n_estimators'])}, \"\n",
    "      f\"learning_rate={best_params['learning_rate']}\")\n",
    "\n",
    "# Retrain with the best parameters\n",
    "best_xgb_params = {\n",
    "    \"max_depth\": int(best_params[\"max_depth\"]),\n",
    "    \"n_estimators\": int(best_params[\"n_estimators\"]),\n",
    "    \"learning_rate\": best_params[\"learning_rate\"],\n",
    "    \"scale_pos_weight\": scale_pos_weight,\n",
    "    \"eval_metric\": \"auc\",\n",
    "    \"objective\": \"binary:logistic\",\n",
    "    \"random_state\": 42,\n",
    "    \"n_jobs\": -1,\n",
    "}\n",
    "\n",
    "best_model = xgb.XGBClassifier(**best_xgb_params)\n",
    "best_model.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "\n",
    "y_prob_best = best_model.predict_proba(X_test)[:, 1]\n",
    "y_pred_best = best_model.predict(X_test)\n",
    "best_auc = roc_auc_score(y_test, y_prob_best)\n",
    "\n",
    "print(f\"\\nFinal AUC-ROC: {best_auc:.4f}\")\n",
    "print(f\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred_best, target_names=[\"Legitimate\", \"Fraud\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1.7 Log to Vertex AI Experiments\n",
    "\n",
    "Track metrics, parameters, and artifacts using the Vertex AI Experiments SDK.\n",
    "This creates a traceable record of every experiment run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform\n",
    "\n",
    "# Initialize Vertex AI\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    ")\n",
    "\n",
    "EXPERIMENT_NAME = \"fraud-detection-exploration\"\n",
    "print(f\"Initializing experiment: {EXPERIMENT_NAME}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create or get the experiment\n",
    "aiplatform.init(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    experiment=EXPERIMENT_NAME,\n",
    ")\n",
    "\n",
    "# Log the best run\n",
    "RUN_NAME = f\"xgb-d{int(best_params['max_depth'])}-n{int(best_params['n_estimators'])}-lr{best_params['learning_rate']}\"\n",
    "\n",
    "with aiplatform.start_run(RUN_NAME) as run:\n",
    "    # Log parameters\n",
    "    run.log_params({\n",
    "        \"max_depth\": int(best_params[\"max_depth\"]),\n",
    "        \"n_estimators\": int(best_params[\"n_estimators\"]),\n",
    "        \"learning_rate\": best_params[\"learning_rate\"],\n",
    "        \"scale_pos_weight\": round(scale_pos_weight, 2),\n",
    "        \"objective\": \"binary:logistic\",\n",
    "        \"num_features\": len(FEATURE_COLS),\n",
    "        \"train_size\": len(X_train),\n",
    "        \"test_size\": len(X_test),\n",
    "        \"split_date\": str(SPLIT_DATE),\n",
    "        \"windows\": str(WINDOWS),\n",
    "    })\n",
    "\n",
    "    # Log metrics\n",
    "    from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "    run.log_metrics({\n",
    "        \"auc_roc\": round(best_auc, 4),\n",
    "        \"precision\": round(precision_score(y_test, y_pred_best), 4),\n",
    "        \"recall\": round(recall_score(y_test, y_pred_best), 4),\n",
    "        \"f1\": round(f1_score(y_test, y_pred_best), 4),\n",
    "        \"train_fraud_rate\": round(y_train.mean(), 4),\n",
    "        \"test_fraud_rate\": round(y_test.mean(), 4),\n",
    "    })\n",
    "\n",
    "    print(f\"Logged run '{RUN_NAME}' to experiment '{EXPERIMENT_NAME}'\")\n",
    "    print(f\"  AUC-ROC: {best_auc:.4f}\")\n",
    "    print(f\"  Precision: {precision_score(y_test, y_pred_best):.4f}\")\n",
    "    print(f\"  Recall: {recall_score(y_test, y_pred_best):.4f}\")\n",
    "    print(f\"  F1: {f1_score(y_test, y_pred_best):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve and display all experiment runs\n",
    "experiment_df = aiplatform.get_experiment_df(EXPERIMENT_NAME)\n",
    "print(f\"All runs for experiment '{EXPERIMENT_NAME}':\")\n",
    "experiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Next Steps\n",
    "\n",
    "Now that we have a working model and tracked our experiments, the next step is to\n",
    "**productionize** this workflow:\n",
    "\n",
    "1. **Refactor into Python modules** -- move the feature engineering and training logic\n",
    "   from this notebook into `fraud_detector/feature_engineering.py` and `fraud_detector/training.py`.\n",
    "\n",
    "2. **Build KFP pipelines** -- wire the modules into Vertex AI Pipelines for automated\n",
    "   training and batch scoring.\n",
    "\n",
    "3. **Set up CI/CD** -- GitHub Actions for testing, staging deployment, and production rollout.\n",
    "\n",
    "4. **Enable monitoring** -- Vertex AI Model Monitoring for data and prediction drift.\n",
    "\n",
    "See the project README for the full walkthrough."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbformat_minor": 4,
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}